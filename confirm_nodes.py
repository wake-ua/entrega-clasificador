import json
from typing import Dict, Any, List, Optional
from langgraph.types import interrupt, Command
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_ollama import ChatOllama
from langgraph.runtime import Runtime

# NOTA: Usamos Any para runtime/state para evitar importaciones circulares con app.py
# Si tienes un archivo shared.py o types.py, imp√≥rtalos desde ah√≠.

# ==========================================
# 1. FUNCIONES AUXILIARES (HELPERS)
# ==========================================

def extract_intent_components(messages: list, llm: ChatOllama) -> Dict[str, Any]:
    """Extrae componentes at√≥micos del intent del usuario."""
    user_messages = [m for m in messages if isinstance(m, HumanMessage)]
    if not user_messages:
        return None
    
    conversation_history = "\n".join([
        f"Usuario: {m.content}" if isinstance(m, HumanMessage) else f"Asistente: {m.content[:100]}"
        for m in messages
    ])
    
    prompt = f"""Analiza la solicitud del usuario y divide su intenci√≥n en componentes estructurados.

MENSAJES:
{conversation_history}

Extrae:
1. topic: Tema principal
2. temporal_filters: Filtros temporales EN LENGUAJE NATURAL
3. demographic_filters: Filtros demogr√°ficos EN LENGUAJE NATURAL
4. spatial_filters: Filtros geogr√°ficos EN LENGUAJE NATURAL
5. required_columns: Columnas mencionadas
6. aggregation_type: Tipo de agregaci√≥n

IMPORTANTE: Responde √öNICAMENTE con un objeto JSON v√°lido, sin explicaciones ni texto adicional.

Formato requerido:
{{
  "topic": "empleo",
  "temporal_filters": ["√∫ltimos 5 a√±os"],
  "demographic_filters": ["mayores de 50 a√±os"],
  "spatial_filters": ["en Espa√±a"],
  "required_columns": ["edad", "fecha", "empleo"],
  "aggregation_type": "statistics"
}}"""
    
    try:
        response = llm.invoke(prompt).content.strip()
        print(f"üîç Respuesta raw del LLM:\n{response[:200]}...")
        
        # Limpiar markdown
        if "```json" in response:
            response = response.split("```json")[1].split("```")[0].strip()
        elif "```" in response:
            response = response.split("```")[1].split("```")[0].strip()
        
        # Intentar parsear JSON
        parsed = json.loads(response)
        print(f"‚úÖ JSON parseado correctamente")
        return parsed
        
    except json.JSONDecodeError as e:
        print(f"‚ùå Error parseando JSON: {e}")
        print(f"Respuesta problem√°tica: {response[:300]}")
        # Retornar un intent b√°sico por defecto
        return {
            "topic": "consulta general",
            "temporal_filters": [],
            "demographic_filters": [],
            "spatial_filters": [],
            "required_columns": [],
            "aggregation_type": "statistics"
        }
    except Exception as e:
        print(f"‚ùå Error inesperado extrayendo componentes: {e}")
        return None

def detect_ambiguities(intent: Dict[str, Any], llm: ChatOllama) -> Optional[str]:
    """Detecta ambig√ºedades o informaci√≥n faltante cr√≠tica."""
    prompt = f"""Analiza si esta b√∫squeda es ambigua onecesita m√°s detalles:

INTENT ACTUAL:
{json.dumps(intent, indent=2, ensure_ascii=False)}

TENEMOS AMBIG√úEDAD CUANDO:
1. B√öSQUEDA DEMASIADO GENERAL: No se especifica D√ìNDE (spatial_filters) ni CU√ÅNDO (temporal_filters), etc. DEBES comprobar que el intent tiene valor para cada filtro. En caso contrario, DEBES preguntar por los vac√≠os.
3. T√âRMINOS VAGOS: Palabras como "reciente", "actual", "√∫ltimos a√±os", "personas mayores", "crisis" que no son concretas.

SI ES AMBIGUO O FALTA CONTEXTO:
Genera preguntas amables y directas para guiar al usuario a completar los filtros.
Ejemplo: "¬øTe interesan datos de un pa√≠s o regi√≥n espec√≠fica?" o "¬øBuscas datos de este a√±o o una serie hist√≥rica?"

IMPORTANTE: Responde SOLO con:
1. Una pregunta corta y amigable, O
2. Exactamente la palabra "NO_AMBIGUITIES"

NO incluyas explicaciones, an√°lisis ni introducciones."""
    
    try:
        response = llm.invoke(prompt).content.strip()
        if "NO_AMBIGUITIES" in response.upper():
            return None
        return response
    except Exception as e:
        return None

def build_confirmation_message(intent: Dict[str, Any], llm: ChatOllama) -> str:
    """Construye mensaje de confirmaci√≥n en primera persona."""
    prompt = f"""Genera un mensaje de confirmaci√≥n EN PRIMERA PERSONA basado en este intent:
{json.dumps(intent, indent=2, ensure_ascii=False)}

Ejemplo: "En resumen, busco datos de empleo en Espa√±a..."
Termina preguntando si es correcto."""
    try:
        return llm.invoke(prompt).content.strip()
    except Exception:
        return f"En resumen, busco datos de {intent.get('topic', 'tu consulta')}. ¬øEs correcto?"

# ==========================================
# 2. NODOS DEL PROCESO DE CONFIRMACI√ìN
# ==========================================

def node_analyze_intent(state: Dict, runtime: Runtime) -> Command:
    """
    NODO 1: L√ìGICA PURA. Analiza y decide el siguiente paso.
    NO contiene interrupt(), por lo que si se re-ejecuta es seguro.
    """
    print("\n--- Entrando en node_analyze_intent ---")
    iterations = state.get("iterations", 0) + 1
    max_iterations = state.get("max_iterations", 15)
    
    if iterations >= max_iterations:
        return Command(
            update={"messages": [AIMessage(content="L√≠mite de pasos alcanzado.")], "iterations": iterations},
            goto="dashboard" # O salida de error
        )

    # 1. Extraer componentes
    print("üîç Analizando intent...")
    intent_components = extract_intent_components(state["messages"], runtime.context.llm)
    
    if not intent_components:
         return Command(
            update={"messages": [AIMessage(content="No entend√≠ tu solicitud. ¬øReformulamos?")], "iterations": iterations},
            goto="chatbot" # O terminar
        )

    # 2. Detectar ambig√ºedades
    clarification = detect_ambiguities(intent_components, runtime.context.llm)
    
    if clarification:
        print("‚ö†Ô∏è Ambig√ºedad detectada. Derivando a pregunta.")
        return Command(
            update={
                "messages": [AIMessage(content=clarification)],
                "iterations": iterations
            },
            goto="ask_clarification"  # Salta al nodo de pregunta
        )
    
    # 3. Preparar confirmaci√≥n
    print("‚úÖ Intent claro. Preparando confirmaci√≥n.")
    confirmation_msg = build_confirmation_message(intent_components, runtime.context.llm)
    
    return Command(
        update={
            "messages": [AIMessage(content=confirmation_msg)],
            "user_search_intent_structured": intent_components,
            "iterations": iterations
        },
        goto="ask_confirmation"  # Salta al nodo de confirmaci√≥n
    )

def node_ask_clarification(state: Dict) -> Command:
    """
    NODO 2: PREGUNTA (Ambig√ºedad).
    Tiene el interrupt al inicio. Al reanudar, no repite l√≥gica pesada.
    """
    print("\n--- Entrando en node_ask_clarification ---")
    
    # Recuperar la √∫ltima pregunta (generada por node_analyze_intent)
    last_msg = state["messages"][-1]
    
    # --- PAUSA ---
    user_response = interrupt(last_msg)
    
    print(f"‚úÖ Respuesta recibida: {user_response}")
    
    # Regresar al an√°lisis con la nueva informaci√≥n
    return Command(
        update={
            "messages": [HumanMessage(content=user_response)]
        },
        goto="analyze_intent"
    )

def node_ask_confirmation(state: Dict, runtime: Runtime) -> Command:
    """
    NODO 3: PREGUNTA (Confirmaci√≥n).
    Tiene el interrupt al inicio.
    """
    print("\n--- Entrando en node_ask_confirmation ---")
    
    last_msg = state["messages"][-1]
    
    # --- PAUSA ---
    user_response = interrupt(last_msg)
    
    # Analizar respuesta (Si/No) - Esto es r√°pido y barato
    check_prompt = f"""Analiza la respuesta del usuario a una pregunta de confirmaci√≥n.
    
    Respuesta del usuario: "{user_response}"
    
    CRITERIOS:
    - AFIRMATIVA: Solo si acepta expl√≠citamente (s√≠, claro, vale, ok, correcto).
    - NEGATIVA: Si dice "no", si pide cambios, si a√±ade informaci√≥n nueva, o si dice algo diferente a confirmar.
    
    Responde SOLO una palabra: "AFIRMATIVA" o "NEGATIVA"."""
    
    decision = runtime.context.llm.invoke(check_prompt).content.strip().upper()
    print(f"ü§î Decisi√≥n del LLM sobre la confirmaci√≥n: {decision}")
    
    if "AFIRMATIVA" in decision:
        print("üöÄ Confirmado. Pasando a b√∫squeda.")
        intent_struct = state.get("user_search_intent_structured", {})
        topic = intent_struct.get("topic", "consulta")
        
        return Command(
            update={
                "messages": [HumanMessage(content=user_response)],
                "user_search_intent": f"Datos de {topic} con filtros confirmados"
            },
            goto="search"  # AVANZA al siguiente paso l√≥gico del grafo
        )
    else:
        print("üîÑ Correcci√≥n detectada. Volviendo a analizar.")
        return Command(
            update={
                "messages": [HumanMessage(content=user_response)]
            },
            goto="analyze_intent"  # RETROCEDE para re-analizar
        )